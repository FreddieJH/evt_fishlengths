
```{r}
\*--- title: "Statistical methods for estimating the maximum body length of fish species" format: html: theme: minty editor: visual execute: echo: false warning: false error: false message: false bibliography: references.bib ---

```{r packages}
library(tidyverse)
library(evd)
library(patchwork)
library(geomtextpath)
library(multidplyr)
library(parallel)
```

# Background

Extreme Value Theory (EVT) is a branch of statistics that focuses on the extreme deviations from the median of probability distributions. Traditionally, EVT has been applied in fields such as hydrology, finance, and environmental sciences to model and predict rare events, such as floods, market crashes, and extreme weather conditions.

In this analysis, we extend the application of EVT to estimate the maximum lengths of fish species. While previous studies, such as @formacion1991, have utilized the Gumbel distribution for this purpose, we aim to incorporate the broader Extreme Value Distribution (EVD) framework. This approach allows for a more comprehensive analysis by considering different types of extreme value distributions, including the Gumbel, FrÃ©chet, and Weibull distributions.

The Generalized Extreme Value (GEV) distribution is defined as:

$$
G(x; \mu, \sigma, \xi) = exp(-[1 + \xi(\frac{x-\mu}{\sigma})]^{-1/\xi})
$$

where:

$\mu$ is the location parameter, $\sigma$ is the scale parameter, $\xi$ is the shape parameter.

By applying the GEV distribution, we can better estimate the maximum lengths of fish species, providing valuable insights for ecological and biological studies.

The Gumbel distribution is a simplified variation of the GEV distribution where $\xi = 0$.

# Understanding GEV

## Shape of GEV

The shape of the GEV distribution is defined by the shape ($\xi$) parameter. Below, location ($\mu$) is fixed at zero, and scale ($\sigma$) is fixed at one.

```{r gev-background}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: Varying the shape parameter of the Generalised Extreme Value (GEV) distribution. The location and scale parameters are fixed at 0 and 1, respectively.
#| label: fig-GEVshape


tibble(x = seq(-4, 4, by = 0.001)) %>% 
  mutate(shape_0 = dgev(x = x, loc=0, scale=1, shape=0),
         shape_neghalf = dgev(x = x, loc=0, scale=1, shape=-0.5),
         shape_half = dgev(x = x, loc=0, scale=1, shape=0.5)) %>% 
  pivot_longer(cols = contains("shape_")) %>% 
  filter(value > 0) %>% 
  ggplot() +
  aes(x = x, y = value, col = name) +
  geom_path(linewidth = 2) +
  labs(x = "x", 
       y = "Density", 
       col = element_blank()) +
  scale_color_discrete(label = c(shape_0 = expression(paste(~xi, " = 0")),
                                 shape_half = expression(paste(~xi, " = 1/2")),
                                 shape_neghalf = expression(paste(~xi, " = -1/2")))) +
  theme_classic(20) +
  theme(legend.position = "inside",
        legend.position.inside = c(1,1), 
        legend.justification = c(1,1))

```

## Simulation of maxima

We can test the GEV distribution with simulated data.

If we take a normal distribution with a mean of 0 and standard deviation of 1, and take a sample of length 1000, and calculate the maximum of this.

```{r norm-max}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: A sample of length 1000 from a normal distribution with a mean of 0 and standard deviation of 1. The sampled values are indicated as the vertical lines along the x-axis, the maximum of this sample is indicated in red.
#| label: fig-dnorm

sample1000 <- tibble(x = rnorm(1000, mean = 0, sd = 1))

tibble(x = seq(-3.5, 3.5, by = 0.01)) |> 
  mutate(y = dnorm(x, mean = 0, sd = 1)) |> 
  ggplot(aes(x, y)) +
  geom_line() +
  geom_rug(y = 1, 
           data = sample1000 |> filter(x == max(x)), 
           col = "red", linewidth = 2, alpha = 0.8) +
  geom_rug(y = 1, data = sample1000) +
  labs(x = "x", 
       y = "Density") +
  theme_classic(20)


```

Given 100 samples, each of length 1000, from a normal distribution ($\mu$ = 0, $\sigma$ = 1), we take the maxima of those 100 samples and look at the distribution of those 100 maxima. According to theory, they should follow the GEV distribution.

```{r gev-understanding}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: The distribution of 100 sample maxima, that are derived from 100 samples of length 1000 from a normal distribution with a mean and standard deviation of 0 and 1, respectively.
#| label: fig-maxima-distribution

tibble(sample_number = 1:100) %>%
  expand_grid(sample_id = 1:1000) %>% 
  mutate(x = rnorm(n = 100*1000)) %>% 
  summarise(sample_maxima = max(x), 
            .by = sample_number) %>% 
  ggplot(aes(x = sample_maxima)) +
  geom_density() +
  labs(x = "Sample maximum", 
       y = "Density") +
  theme_classic(20)


```

Since there is variation in the sampling, we can run this multiple times, each with a new seed, to see how much variation there is.

```{r gev-understanding-multiple}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: The distribution of 100 sample maxima, repeated 100 times. 
#| label: fig-maxima-distribution-multiple

for(seed in 1:100){
  
  if(seed == 1) out_tab <- tibble()
  
  set.seed(seed)
  
  tab <- 
    tibble(sample_number = 1:100) %>%
    expand_grid(sample_id = 1:1000) %>% 
    mutate(x = rnorm(n = 100*1000)) %>% 
    summarise(sample_maxima = max(x), 
              .by = sample_number) %>% 
    mutate(seed = seed)
  
  out_tab <- 
    bind_rows(
      out_tab, 
      tab
    )
  
}

out_tab %>% 
  ggplot(aes(x = sample_maxima, 
             group = seed)) +
  geom_density(colour=alpha("red", 0.3)) +
  labs(x = "Sample maximum", 
       y = "Density") +
  theme_classic(20)


```

## Fitting GEV to simulated maxima

```{r}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: Fitting 100 GEV distributions to 100 repeats, each of length 100 sample maxima. 
#| label: fig-GEVfitting-multiple

gev_fits <- 
  out_tab %>% 
  nest(.by = seed) %>% 
  mutate(gev_fit = map(.x = data, 
                       .f = ~evd::fgev(.x$sample_maxima)), 
         loc = map_dbl(.x = gev_fit,
                       .f = ~.x$estimate["loc"]),
         scale = map_dbl(.x = gev_fit,
                         .f = ~.x$estimate["scale"]),
         shape = map_dbl(.x = gev_fit,
                         .f = ~.x$estimate["shape"])) 

median_gev_fits <-
  gev_fits %>% 
  summarise(med_loc = median(loc), 
            med_scale = median(scale), 
            med_shape = median(shape))


median_gev_ests <- 
  tibble(x = seq(min(out_tab$sample_maxima), 
                 max(out_tab$sample_maxima), 
                 by = 0.001), 
         y = dgev(x = x, 
                  loc = median_gev_fits$med_loc, 
                  scale = median_gev_fits$med_scale,
                  shape = median_gev_fits$med_shape))

ci_gev_ests <- 
  gev_fits %>% 
  select(seed, loc, scale, shape) %>% 
  expand_grid(x = seq(min(out_tab$sample_maxima), 
                      max(out_tab$sample_maxima), 
                      by = 0.001)) %>% 
  rowwise() %>%
  mutate(y = dgev(x = x, 
                  loc = loc, 
                  scale = scale,
                  shape = shape)) %>% 
  ungroup() %>% 
  summarise(
    median = quantile(y, p = 0.5),
    lower = quantile(y, p = 0.025),
    upper = quantile(y, p = 0.975), 
    .by = x
  )



out_tab %>% 
  ggplot(aes(x = sample_maxima, 
             group = seed)) +
  geom_density(colour=alpha("red", 0.3)) +
  
  geom_ribbon(inherit.aes = FALSE, 
              aes(x = x, ymax = upper, ymin = lower), 
              data = ci_gev_ests, 
              linewidth = 1, 
              alpha = 0.3) +
  geom_line(inherit.aes = FALSE, 
            aes(x = x, y = median), 
            data = ci_gev_ests, 
            linewidth = 1) +
  labs(x = "Sample maximum", 
       y = "Density") +
  theme_classic(20)

```

# Estimating maxima (EVT)

The plots show that sample maxima can be well described by the GEV distribution. The following question from this, is how can we use the fitted GEV distribution to estimate the maximum of the underlying distribution.

For this we will use a slightly more realistic example of the underlying 'true' population in relation to fish body lengths.

The underlying 'true' distribution of body lengths within a population will be defined by a normal distribution with a mean of 50 (cm) and a standard deviation of 16 (approximately one third of 50 - see Heather et al, *in review*).

We will take 10 samples each of length 1000 from the underlying population, to get 10 sample maxima to estimate the GEV distribution.

```{r}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: Estimating maximum length from 10 sample maxima. The sample maxima are calculated from samples of length 1000. 
#| label: fig-estimate-lmax

l_mean <- 50 #cm
l_sd <- 16 
n_samples <- 10
sample_size_mean <- 1000

for(s in 1:n_samples){
  if(s == 1) {
    sample_maxima <- c()
    sample_sizes <- c()
  }
  set.seed(s)
  sample_size <- round(rnorm(1, sample_size_mean, 300))
  sample <- rnorm(sample_size, mean = l_mean, sd = l_sd)
  sample_maxima[s] <- max(sample)
  sample_sizes[s] <- sample_size
}

gev_fit <- suppressWarnings(try(evd::fgev(x = sample_maxima),
                                    silent = TRUE))

gev_loc <- gev_fit$estimate["loc"]
gev_scale <- gev_fit$estimate["scale"]
gev_shape <- gev_fit$estimate["shape"]
gev_loc_se <- gev_fit$std.err["loc"]
gev_scale_se <- gev_fit$std.err["scale"]
gev_shape_se <- gev_fit$std.err["shape"]

qgev_est <- function(pc) qgev(pc, gev_loc, gev_scale, gev_shape)
qgev_upr <- function(pc) qgev(pc, gev_loc + gev_loc_se, 
                              gev_scale + gev_scale_se, 
                              gev_shape + gev_shape_se)
qgev_lwr <- function(pc) qgev(pc, gev_loc - gev_loc_se, 
                              gev_scale - gev_scale_se, 
                              gev_shape - gev_shape_se)

pgev_est <- function(q) pgev(q, gev_loc, gev_scale, gev_shape)
pgev_upr <- function(q) pgev(q, gev_loc + gev_loc_se, 
                              gev_scale + gev_scale_se, 
                              gev_shape + gev_shape_se)
pgev_lwr <- function(q) pgev(q, gev_loc - gev_loc_se, 
                              gev_scale - gev_scale_se, 
                              gev_shape - gev_shape_se)


qnorm_99 <- qnorm(0.9900, mean = l_mean, sd = l_sd)
qnorm_999 <- qnorm(0.9990, mean = l_mean, sd = l_sd)
qnorm_9999 <- qnorm(0.9999, mean = l_mean, sd = l_sd)
qnorm_99999 <- qnorm(0.99999, mean = l_mean, sd = l_sd)
qgev_99 <- qgev_est(0.99)

pc <- n_samples/(n_samples+1)
qgev_better <- qgev_est(pc)
qgev_lwr(pc)
qgev_upr(pc)

p1 <-
  ggplot() +
      geom_textdensity(aes(x), data = tibble(x = rnorm(1e6, 
                                                   mean = l_mean, 
                                                   sd = l_sd)) |> filter(x > 0), 
                       label = "Underlying body size distribution", hjust = 0.2) +
      geom_rug(aes(x = sample_maxima), 
               color = "purple", 
               length = unit(0.5, units = "cm")) +
  geom_rect(aes(xmin = qgev_lwr(n_samples/(n_samples+1)),
                xmax = qgev_upr(n_samples/(n_samples+1)),
                ymin = -Inf, 
                ymax = Inf), 
            fill = "grey70", 
            alpha = 0.3) +
  
    geom_textvline(aes(xintercept = qgev_better), 
                   color = "purple", 
                   linetype = "dashed", 
                   label = "Estimated LMAX") +
    labs(x = "Fish body length",
         y = "Frequency") +
    theme_minimal()

p1_lims <- layer_scales(p1)$x$range$range

plot_data <-
      tibble(max = sample_maxima, 
             rank = rank(sample_maxima), 
             pos = rank/(max(rank)+1), 
             n = sample_sizes) 

  p2 <-
      tibble(x = seq(p1_lims[1], p1_lims[2], by = 0.1)) %>% 
  mutate(pgev = pgev(q = x,
                     loc = gev_loc,
                     scale = gev_scale,
                     shape = gev_shape),
         pgev_lwr = pgev(q = x,
                     loc = gev_loc - gev_loc_se,
                     scale = gev_scale - gev_scale_se,
                     shape = gev_shape - gev_shape_se),
         pgev_upr = pgev(q = x,
                     loc = gev_loc + gev_loc_se,
                     scale = gev_scale + gev_scale_se,
                     shape = gev_shape + gev_shape_se)) |> 
      ggplot() +
    geom_ribbon(aes(x = x, ymin = pgev_lwr, ymax = pgev_upr),
               fill = "grey70", alpha = 0.3) +
      geom_point(aes(x = max, y = pos, size = n), color = "purple",
                 plot_data) +
      geom_line(aes(x = x, y = pgev)) +
    geom_errorbarh(aes(xmin = qgev_lwr(pc), 
                   xmax = qgev_upr(pc), 
                   y = pc),
                   height = 0.03,
               col = "purple", 
               lty = "dashed", 
               data = tibble(n = 1)) +
      geom_vline(xintercept = qgev_better, col = "purple", lty = "dashed") +
     labs(x = "Fish body length",
         y = "Probability density", 
         size = "Sample size") +
      theme_minimal() +
    theme(legend.position = c(0.05, 1), 
          legend.justification = c(0,1.2), 
          legend.background = element_rect(fill = "white", 
                                           color = "black")) 
  
p3 <- p1+p2 + plot_layout(ncol = 1) + plot_annotation(tag_levels = 'A')

ggsave("simulated_evt.png",
       plot = p3,
       height = 8,
       width = 8)
    
```

In this simple case the estimated maximum length is determined by the sampling effort. The greater the sampling effort, the more likely you are to observe extreme length measures. It is therefore important to account for the sampling effort in the estimation to be clear as to what it is you are estimating. Using 10 sample maxima, each derived from a sample size of 1000, we observe the estimated LMAX to be between the 99.9th percentile and the 99.99th percentile. If we have larger sample sizes, it can be expected that this sample size increases.

We can see that increasing the size of the sample by 10-fold, increases the percentile being estimated by a similar 10-fold proportion (e.g. 99th to 99.9th percentile).

```{r}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: The size of the samples that the maximum is derived is important. The number of samples that you have is also important, but this would be known.
#| label: fig-percentile-estimated

l_mean <- 50 #cm
l_sd <- 16.67 #cm

fit_gev <- function(n_samples, sample_size, l_mean, l_sd) {
  
  # Generate samples and find the maxima
  sample_maxima <- map_dbl(1:n_samples, ~max(rnorm(sample_size, mean = l_mean, sd = l_sd)))
  
  gev_fit <- suppressWarnings(try(evd::fgev(x = sample_maxima), silent = TRUE))
  
  if (inherits(gev_fit, "try-error")) {
    return(c(loc = 0, scale = 0, shape = 0))
  }
  
  return(c(loc = gev_fit$estimate["loc"], scale = gev_fit$estimate["scale"], shape = gev_fit$estimate["shape"]))
}

n_cores <- parallel::detectCores() - 2
cluster <- new_cluster(n_cores)
cluster_library(cluster, "purrr")
cluster_library(cluster, "evd")
cluster_copy(cluster, "fit_gev")
cluster_copy(cluster, "qgev")
cluster_copy(cluster, "tibble")
cluster_send(cluster, l_mean <-  50)
cluster_send(cluster, l_sd <-  16.67)

sim_output <- 
  expand_grid(ns = c(10, 20, 50, 100, 200, 500, 1000, 2000), 
            ss = c(10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000), 
            rep = 1:10) |> 
  partition(cluster) |>
  mutate(out = pmap_dfr(.l = list(ns = ns, ss = ss), 
                        .f = function(ns, ss) {
    gev_params <- fit_gev(ns, ss, l_mean = l_mean, l_sd = l_sd) 
    tibble(loc = gev_params[1], scale = gev_params[2], shape = gev_params[3])
  })) |> 
  collect() |> 
  unnest(cols = out)

sim_output |> 
  filter(loc != 0) |> 
  rowwise() |> 
  mutate(lmax_est = qgev(ns/(ns+1), loc = loc, scale = scale, shape = shape)) |>
  filter(lmax_est < l_mean*3) |> 
  ggplot() +
  aes(x = ss, y = lmax_est, col = as.factor(ns)) +
  geom_point() + 
  stat_smooth() +
  geom_texthline(aes(yintercept = yi), 
                 label = "99th percentile", 
                 data = tibble(yi = qnorm_99)) +
  geom_texthline(aes(yintercept = yi), 
                 label = "99.9th percentile", 
                 data = tibble(yi = qnorm_999)) +
  geom_texthline(aes(yintercept = yi), 
                 label = "99.99th percentile", 
                 data = tibble(yi = qnorm_9999)) +
  geom_texthline(aes(yintercept = yi), 
                 label = "99.999th percentile", 
                 data = tibble(yi = qnorm_99999)) +
  scale_x_log10() +
  labs(x = "Sample size (log axis)", 
       y = "Lmax estimation (cm)", 
       col = "# samples") +
  theme_minimal()


```

## Accounting for sampling effort

It is very difficult to try to estimate the sampling effort. In our case, the sample maxima likely come from various sources (e.g. fishing competitions), that may have very different efforts. What we can do is estimate the maximum length based on some arbitrary number of fishing competitions, e.g. expected maximum to be caught for 100 fishing competitions.

```{r}
#| fig-width: 8
#| fig-asp: 0.65
#| fig-align: center
#| fig-cap: The estimated maximum length given 10, 100, or 1000 sampling efforts (e.g. fishing competitions), as indicated by the yellow, red, and blue vertical dashed lines respectively. 
#| label: fig-number-efforts


# qgev_10comps <- qgev_est(10/(10+1))
# qgev_100comps <- qgev_est(100/(100+1))
# qgev_1000comps <- qgev_est(1000/(1000+1))

qgev_10comps <- qgev_est(1-(1/10))
qgev_100comps <- qgev_est(1-(1/100))
qgev_1000comps <- qgev_est(1-(1/1000))

tibble(x = seq(80, 120, by = 0.1)) %>% 
  mutate(pgev = pgev(q = x, 
                     loc = gev_loc, 
                     scale = gev_scale,
                     shape = gev_shape)) %>% 
  ggplot() +
  geom_point(aes(x = max, y = pos), color = "purple",
             plot_data) +
  geom_line(aes(x = x, y = pgev)) +
  geom_vline(xintercept=qgev_10comps, col = "orange", lty = "dashed") +
  geom_vline(xintercept=qgev_100comps, col = "darkred", lty = "dashed") +
  geom_vline(xintercept=qgev_1000comps, col = "navy", lty = "dashed") +
  # geom_vline(xintercept = pc, col = "purple", lty = "dashed") +
  labs(x = "Fish body length",
       y = "Probability") +
  theme_minimal()

```

## Applying EVT to real data (snapper)

Using the evd package to fit our distribution means that we can extract the standard error in the estimates of the three parameters.

```{r}

kg2cm <- function(w, a = 0.01, b = 3) ((w*1000)/a)^(1/3)

snapper_maxima <- c(91.3, 
            kg2cm(11.8),
            102, 112, 
            kg2cm(18.4),
            kg2cm(16.5),
            107, 107, 99.2 
            , 95
            , 82.2
            ,kg2cm(17.2)
            )
quant_100comps <- 1-(1/100)

gev_fit <- suppressWarnings(try(evd::fgev(x = snapper_maxima),
                                    silent = TRUE))

plot_data <-
      tibble(max = snapper_maxima, 
             rank = rank(snapper_maxima), 
             pos = rank/(max(rank)+1)) 

gev_loc <- gev_fit$estimate["loc"]
gev_scale <- gev_fit$estimate["scale"]
gev_shape <- gev_fit$estimate["shape"]
qgev_est <- function(pc) qgev(pc, gev_loc, gev_scale, gev_shape)


gev_loc_se <- gev_fit$std.err["loc"]
gev_scale_se <- gev_fit$std.err["scale"]
gev_shape_se <- gev_fit$std.err["shape"]

lmax_100comps_fit <- qgev(quant_100comps, gev_loc, gev_scale, gev_shape)
lmax_100comps_lwr <- qgev(quant_100comps,
                          gev_loc - gev_loc_se,
                          gev_scale - gev_scale_se,
                          gev_shape - gev_shape_se)
lmax_100comps_upr <- qgev(quant_100comps,
                          gev_loc + gev_loc_se,
                          gev_scale + gev_scale_se,
                          gev_shape + gev_shape_se)

add_line <- function(val, val2, lty){
  geom_textsegment(
    y = -Inf,
    yend = val2,
    x = val,
    xend = val,
    lty = lty,
                 label = paste0(round(val), "cm"),
                 hjust = 0.1,
                 col = "darkred",
                 lty = "dashed",
    data = tibble(n = 1))
}



tibble(x = seq(min(snapper_maxima)*0.8, max(snapper_maxima)*1.3, by = 0.1)) %>%
  mutate(pgev = pgev(q = x,
                     loc = gev_loc,
                     scale = gev_scale,
                     shape = gev_shape),
         pgev_lwr = pgev(q = x,
                     loc = gev_loc - gev_loc_se,
                     scale = gev_scale - gev_scale_se,
                     shape = gev_shape - gev_shape_se),
         pgev_upr = pgev(q = x,
                     loc = gev_loc + gev_loc_se,
                     scale = gev_scale + gev_scale_se,
                     shape = gev_shape + gev_shape_se)) %>%
  ggplot() +
     geom_ribbon(aes(x = x, ymin = pgev_lwr, ymax = pgev_upr),
               fill = "grey70", alpha = 0.3) +
  geom_point(aes(x = max, y = pos), color = "purple",
             plot_data) +
  geom_line(aes(x = x, y = pgev)) +
  add_line(lmax_100comps_fit, quant_100comps, 1) +
  add_line(lmax_100comps_lwr, quant_100comps, 2) +
  add_line(lmax_100comps_upr, quant_100comps, 2) +
  geom_textsegment(x = -Inf, xend = lmax_100comps_upr,
               y = quant_100comps, yend = quant_100comps,
               col = "darkred",
               label = "if number of samples = 100",
               data = tibble(n = 1),
               hjust = 0.1) +
    annotate("text", 
           x = -Inf, 
           y = 0.9, 
           family = "Fira mono",
           label = sprintf("Î¼ = %-6.1f\nÏ = %-6.1f\nÎ¾ = %-6.3f", gev_loc, gev_scale, gev_shape),
           # label = paste(sprintf("%-1s = %.1f", "Î¼", gev_loc), "\n",
           #               sprintf("%-1s = %.1f", "Ï", gev_scale), "\n",
           #               sprintf("%-1s = %.3f", "Î¾", gev_shape)), 
           # label = paste("Î¼ =", round(gev_loc, 1), "\n",
           #               "Ï =", round(gev_scale, 1), "\n",
           #               "Î¾ =", round(gev_shape, 2)), 
           size = 6, 
           hjust = -0.5, vjust = 1) +
  labs(x = "Fish body length (cm)",
       y = "Probability") +
  theme_minimal()

ggsave("snapper_evt.png",
       height = 5,
       width = 5*1.6)

```

# Numerical estimation of $L_{max}$

```{r}
library(furrr)
library(cmdstanr)
library(data.table)
library(tidyverse)


sim_pois_truncnorm <- function(k, n_lambda, mu, sigma, min_size) {
    n <- rpois(k, lambda = n_lambda) # number of fish per competition
    maxima <- numeric(k)
    for (i in seq_len(k)) {
        maxima[i] <- max(
            truncnorm::rtruncnorm(
                n = n[i],
                a = min_size,
                mean = mu,
                sd = sigma
            )
        )
    }
    return(maxima)
}


# simulation over all scenarios
sim_tbl <-
    expand_grid(
        rep = 1:10,
        k = c(3, 5, 10, 20, 30, 100),
        n_lambda = c(30, 50, 200, 2000, 10000),
        mu = c(20, 50, 100),
    ) |>
    mutate(
        sigma = mu * 0.34,
        min_size = 10,
        filename = paste0(
            "rep", rep,
            "_k", k,
            "_lambda", n_lambda,
            "_mu", mu
        )
    )

{
    plan(multisession)
    sim_tbl_max <-
        sim_tbl %>%
        mutate(maxima = future_pmap(
            list(k, n_lambda, mu, sigma, min_size),
            sim_pois_truncnorm,
            .options = furrr_options(seed = TRUE)
        ))
    plan(sequential)
}

mod <- cmdstan_model("max_est.stan", stanc_options = list("O1"))

dir.create("stan_outputs", showWarnings = FALSE, recursive = TRUE)
dir.create("max_posterior", showWarnings = FALSE, recursive = TRUE)

# Stan fitting function
fit_stan_model <- function(rep, k, lambda, mu, maxima) {
    fit <- mod$sample(
        data = list(k = length(maxima), x = maxima),
        iter_warmup = 1000,
        iter_sampling = 1000,
        chains = 4,
        parallel_chains = 4,
        refresh = 500
    )

    filename <-
        paste0(
            "rep", rep,
            "_k", k,
            "_lambda", lambda,
            "_mu", mu
        )

    est_max_tbl <-
        fit$draws(format = "df") |>
        select(mu, sigma, lambda) |>
        mutate(est_max = mu + sigma * qnorm(lambda / (lambda + 1)))

    data.table::fwrite(fit$summary(), file = paste0("stan_outputs/", filename, ".csv"))
    data.table::fwrite(est_max_tbl, file = paste0("max_posterior/", filename, ".csv"))

    return(NULL)
}

processed_files <-
    list.files(
        "stan_outputs",
        full.names = FALSE
    ) %>%
    str_remove(".csv")

unprocessed_tbl <-
    sim_tbl_max |>
    filter(!filename %in% processed_files)

{
    plan(multisession)
    future_pmap(
        .l = list(
            rep = unprocessed_tbl$rep,
            k = unprocessed_tbl$k,
            lambda = unprocessed_tbl$n_lambda,
            mu = unprocessed_tbl$mu,
            maxima = unprocessed_tbl$maxima
        ),
        .f = fit_stan_model,
        .options = furrr_options(seed = TRUE),
        .progress = TRUE
    )
    plan(sequential)
}

combine_stan_results <- function() {
    filename_pattern <- "rep[0-9]+_k[0-9]+_lambda[0-9]+_mu[0-9]+"
    vroom::vroom(
        list.files("stan_outputs",
            pattern = filename_pattern,
            full.names = TRUE
        ),
        id = "filename",
        num_threads = parallel::detectCores() - 2
    ) %>%
        mutate(filename = str_extract(filename, filename_pattern)) %>%
        left_join(sim_tbl) %>%
        fwrite(file = "stan_output.csv")
}

if (file.exists("stan_output.csv")) {
    saved_files <-
        vroom::vroom("stan_output.csv") %>%
        pull(filename) %>%
        unique() %>%
        str_remove(".csv")

    unsaved_files <-
        sim_tbl_max %>%
        filter(!filename %in% saved_files) %>%
        pull(filename)

    if (length(unsaved_files)) {
        combine_stan_results()
    }
} else {
    combine_stan_results()
}

sim_output <- vroom::vroom("stan_output.csv")


```

The method appears to estimate the maximum length well, even if it does not estimate the exact value of the underlying distribution perfectly. This makes sense, considering a large fish may be caught either if the population has a large mean size, or if the sample size is large with a relatively smaller mean body size.

```{r}
  sim_output |> 
  select(filename, variable, median) %>% 
    pivot_wider(names_from = variable, 
                values_from = median, 
                names_prefix = "est_") |> 
                left_join(sim_tbl) %>% 
    mutate(est_max = est_mu + (est_sigma*qnorm(1 - 1/est_lambda)), 
           calc_max = mu + (sigma*qnorm(1 - 1/n_lambda))) |> 
    ggplot(aes(est_max, calc_max, 
               col = as.factor(mu), 
               size = as.factor(n_lambda))) + 
    geom_point(alpha = 0.1) +
    geom_abline(slope = 1)


```

## Numerical estimation - real data

```{r}
mod <- cmdstan_model("max_est.stan", stanc_options = list("O1"))
snapper_fit <- mod$sample(
    data = list(
        k = length(snapper_maxima),
        x = snapper_maxima
    ),
    iter_warmup = 1000,
    iter_sampling = 2000,
    chains = 4,
    parallel_chains = 4,
    refresh = 500,
    save_warmup = TRUE
)

snapper_fit_summary <-
    snapper_fit |>
    posterior::summarise_draws()

library(posterior)

snapper_fit %>%
    as_draws_df() %>%
    mutate(est_max = mu + (sigma * qnorm(1 - (1 / lambda)))) %>%
    reframe(
        median_max = median(est_max),
        sd = sd(est_max)
    )

library(cmdstanr)
library(bayesplot)

bayesplot::mcmc_trace(snapper_fit$draws(inc_warmup = TRUE),
    pars = c("mu", "sigma", "lambda"),
    n_warmup = 1000
) + ggplot2::scale_color_discrete() +
    theme(plot.background = element_rect(fill = "white"))

```

```{r}
calc_max <- function(x, n, mu, sigma) {
    fx <- dnorm(x, mu, sigma)
    Fx <- pnorm(x, mu, sigma)
    max <- (n * (Fx^(n - 1))) * fx
    return(max)
}

xx <-
    snapper_fit %>%
    as_draws_df() %>%
    as_tibble() %>%
    expand_grid(size = 0:150) %>%
    mutate(max = calc_max(size, n = lambda, mu = mu, sigma = sigma)) 

xx %>% 
  arrange(desc(max))

xx %>%
    reframe(
        max_fit = mean(max),
        max_lwr = quantile(max, 0.025),
        max_upr = quantile(max, 0.975),
        .by = size
    ) %>%
    ggplot(aes(size, max_fit)) +
    geom_ribbon(aes(ymin = max_lwr, ymax = max_upr),
        fill = "grey70",
        alpha = 0.8
    ) +
    geom_line() +
    theme_classic(20) +
    geom_rug(aes(x = maxima), data = tibble(maxima = snapper_maxima), 
    col = "purple",
    inherit.aes = FALSE) +
    labs(x = "Body size", 
    y = "Pr(size = max_size)")

ggsave("estimated_max_posterior.png", 
width = 10, height = 7)

```

```{r}

expected_max <- function(n, mu, sigma, lower = 0, upper = Inf) {
  integrand <- function(x) {
    x * n * (pnorm(x, mu, sigma)^(n - 1)) * dnorm(x, mu, sigma)
  }
  integrate(integrand, lower, upper)$value
}

emax_tbl <- 
  snapper_fit %>%
    as_draws_df() %>%
    as_tibble()  %>% 
    mutate(emax = pmap_dbl(.l = list(mu = mu, sigma = sigma, n = lambda), 
    .f = expected_max)) 

emax_tbl %>% 
reframe(median = median(emax), 
q5 = quantile(emax, 0.025), 
q95 = quantile(emax, 0.975))

emax_tbl %>% 
    ggplot(aes(x = emax)) +
    geom_density() +
    theme_classic(20) +
    labs(x = "Expected maximum length")

ggsave("expected_max_posterior.png", 
width = 10, height = 7)

```




It is possible that the prior of the model is influencing the estimate of the maximum length. We can therefore try to change the prior on lambda to see how that influences the estimate of the est_max length.

```{r}
modify_lambda <- function(lambda_alpha, lambda_beta) {
    paste0("
data {
  int<lower=1> k;  // Number of competitions
  vector[k] x;  // Recorded largest fish sizes
}

parameters {
  real<lower=0> mu;    // Mean of the fish size distribution
  real<lower=0> sigma; // Standard deviation
  real<lower=1> lambda;     // Estimated mean number of fish caught per competition
}

model {
  // Priors
  mu ~ normal(30, 20);
  sigma ~ normal(10, 5);
  lambda ~ gamma(", lambda_alpha, ",", lambda_beta, ");


  target += k*log(lambda) - lambda*k;

  for (i in 1:k) {

  target += lambda*normal_cdf(x[i] | mu, sigma) + normal_lpdf(x[i]|mu, sigma);

}
}
generated quantities {
  real est_max;
  est_max = mu + sigma * inv_Phi(1 - (1 / lambda));
}
")
}

lambda_sim_tbl <-
    expand_grid(
        lambda_alpha = c(0.5, 5, 50, 500),
        gamma_mean = c(100, 1000, 10000, 100000)
    ) %>%
    mutate(lambda_beta = lambda_alpha / gamma_mean)

run_stan_model <- function(lambda_alpha, lambda_beta) {
    stan_file <- tempfile(fileext = ".stan")
    writeLines(modify_lambda(lambda_alpha, lambda_beta), stan_file)

    mod <- cmdstan_model(stan_file, stanc_options = list("O1"))

    fit <- mod$sample(
        data = list(
            k = length(snapper_maxima),
            x = snapper_maxima
        ),
        iter_warmup = 1000,
        iter_sampling = 2000,
        chains = 4,
        parallel_chains = 4,
        refresh = 500,
        save_warmup = TRUE
    )

    out1 <-
        fit |>
        posterior::summarise_draws() |>
        mutate(
            lambda_alpha = lambda_alpha,
            lambda_beta = lambda_beta
        )
    return(out1)
}
if (!file.exists("snapper_fit_lambda.csv")) {
    plan(multicore)
    output <-
        lambda_sim_tbl %>%
        mutate(fit_summary = future_pmap(
            .l = list(
                lambda_alpha = lambda_alpha,
                lambda_beta = lambda_beta
            ),
            .f = run_stan_model,
            .options = furrr_options(seed = TRUE),
            .progress = TRUE
        ))
    plan(sequential)

    output %>%
        select(fit_summary) %>%
        unnest(cols = c(fit_summary)) %>%
        write_csv("snapper_fit_lambda.csv")
}

lambda_output <- read_csv("snapper_fit_lambda.csv")

```


```{r}

lambda_output %>% 
  filter(variable == "est_max") %>% 
  mutate(gamma_mean = lambda_alpha / lambda_beta) %>% 
  ggplot(aes(x = gamma_mean, y = median)) +
  geom_point(size = 4) +  
  geom_errorbar(aes(ymin = q5, ymax = q95), width = 0.5) +  
  labs(
    x = "Mean of gamma prior on lambda (i.e., sample size in each comp.)",
    y = "Estimated maximum length (model output)"
  ) +
  theme_minimal(base_size = 14)


ggsave("lambda_prior_estmax_snapper.png", 
height = 5, 
width = 5*1.6)

```

Is the estimated maximum always lower than the maxima values that are put into it?

```{r}

custom_labeller <- function(labels) {
  paste0(labels, " competitions (# maxima)")
}

sim_output %>%
    select(filename, variable, median) %>%
    pivot_wider(
        names_from = variable,
        values_from = median,
        names_prefix = "est_"
    ) %>%
    left_join(sim_tbl_max) %>%
    mutate(
        est_max = est_mu + (est_sigma * qnorm(1 - 1 / est_lambda)),
        calc_max = mu + (sigma * qnorm(1 - 1 / n_lambda))
    ) %>%
    select(maxima, est_max, filename, mu, n_lambda, k) %>%
    unnest(cols = c(maxima)) %>%
    ggplot(aes(maxima, est_max, col = as.factor(n_lambda), pch = as.factor(mu))) +
    geom_point(alpha = 0.3) +
    geom_abline(slope = 1, col = "red") +
    facet_wrap(~k, ncol = 2, labeller = as_labeller(custom_labeller)) +
    labs(x = "Maxima values (data input)", 
    y = "Estimated maximum (model output)", 
    shape = "True Mu value (pop. mean body-length)",  
    color = "True lambda value (# samples in comp.)") +
    theme(legend.position = "bottom", legend.box = "vertical")

ggsave("maxima_vs_estmax.png",
    height = 10,
    width = 6
)
```